{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py:2641: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
      "  VisibleDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn.metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import grid_search\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_json('../data/train.json/train.json')\n",
    "train.head()\n",
    "\n",
    "clean_string = [' , '.join(z).strip() for z in train['ingredients']]  \n",
    "ingredients_string = [' '.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]', ' ', line)) for line in lists]).strip() for lists in train['ingredients']]\n",
    "\n",
    "cnt_vec = CountVectorizer(max_features = 2000)\n",
    "tfidf_vec = TfidfVectorizer(stop_words='english',\n",
    "                             ngram_range = ( 1 , 1 ),\n",
    "                             analyzer=\"word\", \n",
    "                             max_df = .57 , \n",
    "                             binary=False , \n",
    "                             token_pattern=r'\\w+', \n",
    "                             sublinear_tf=False)\n",
    "\n",
    "comb_feats = FeatureUnion([(\"cnt\", cnt_vec), (\"tfidf\", tfidf_vec)])\n",
    "\n",
    "ingredients = train['ingredients']\n",
    "words_list = [' '.join(x) for x in ingredients]\n",
    "\n",
    "###test\n",
    "words_list = ingredients_string\n",
    "\n",
    "#Make label encoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train[\"cuisine\"])\n",
    "\n",
    "labels = le.transform(train['cuisine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "bag_of_words = vectorizer.fit(words_list)\n",
    "bag_of_words = vectorizer.transform(words_list).toarray()\n",
    "print(bag_of_words.shape)\n",
    "\n",
    "tfidf_feat = vectorizertr.fit(words_list)\n",
    "tfidf_feat = vectorizertr.transform(words_list).toarray()\n",
    "'''\n",
    "X_features = comb_feats.fit(words_list, labels).transform(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(39774, 4963)\n",
      "(39774,)\n"
     ]
    }
   ],
   "source": [
    "print type(X_features)\n",
    "print X_features.shape\n",
    "print train['cuisine'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "3s - loss: 1.2840 - acc: 0.6226\n",
      "Epoch 2/15\n",
      "3s - loss: 0.8038 - acc: 0.7635\n",
      "Epoch 3/15\n",
      "3s - loss: 0.6430 - acc: 0.8128\n",
      "Epoch 4/15\n",
      "3s - loss: 0.5183 - acc: 0.8470\n",
      "Epoch 5/15\n",
      "3s - loss: 0.4131 - acc: 0.8772\n",
      "Epoch 6/15\n",
      "3s - loss: 0.3272 - acc: 0.9025\n",
      "Epoch 7/15\n",
      "3s - loss: 0.2579 - acc: 0.9226\n",
      "Epoch 8/15\n",
      "3s - loss: 0.1981 - acc: 0.9397\n",
      "Epoch 9/15\n",
      "3s - loss: 0.1582 - acc: 0.9512\n",
      "Epoch 10/15\n",
      "3s - loss: 0.1298 - acc: 0.9603\n",
      "Epoch 11/15\n",
      "3s - loss: 0.1084 - acc: 0.9656\n",
      "Epoch 12/15\n",
      "3s - loss: 0.0963 - acc: 0.9706\n",
      "Epoch 13/15\n",
      "3s - loss: 0.0814 - acc: 0.9749\n",
      "Epoch 14/15\n",
      "3s - loss: 0.0693 - acc: 0.9783\n",
      "Epoch 15/15\n",
      "3s - loss: 0.0638 - acc: 0.9806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe92dab9690>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "batch_size = 128\n",
    "nb_epoch = 15\n",
    "\n",
    "X_train = X_features.todense()\n",
    "y_train = le.transform(train[\"cuisine\"])\n",
    "\n",
    "nb_classes = len(le.classes_)\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "\n",
    "#output_dim=64, input_dim=100,\n",
    "model = Sequential()\n",
    "model.add(Dense(1024,input_shape=(4963,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(.4))\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(.4))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "rms = RMSprop()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=rms)\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          show_accuracy=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now read the test json file in \n",
    "test = pd.read_json('../data/test.json/test.json')\n",
    "test.head()\n",
    "\n",
    "#Do the same thing we did with the training set and create a array using the count vectorizer. \n",
    "test_ingredients = test['ingredients']\n",
    "test_ingredients_words = [' '.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]', ' ', line)) for line in lists]).strip() for lists in test['ingredients']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9944/9944 [==============================] - 0s     \n"
     ]
    }
   ],
   "source": [
    "X_test = comb_feats.transform(test_ingredients_words).todense()\n",
    "result = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = le.inverse_transform(result)\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"cuisine\":result} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv( \"ANN_featunion_model.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traindf = pd.read_json(\"../data/train.json/train.json\")\n",
    "traindf['ingredients_clean_string'] = [' , '.join(z).strip() for z in traindf['ingredients']]  \n",
    "traindf['ingredients_string'] = [' '.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]', ' ', line)) for line in lists]).strip() for lists in traindf['ingredients']]       \n",
    "\n",
    "tmp = [' '.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]', ' ', line)) for line in lists]).strip() for lists in traindf['ingredients']]\n",
    "print type(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print tmp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A combination of Word lemmatization + LinearSVC model finally pushes the accuracy score past 80%\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "corpustr = traindf['ingredients_string']\n",
    "vectorizertr = TfidfVectorizer(stop_words='english',\n",
    "                             ngram_range = ( 1 , 1 ),analyzer=\"word\", \n",
    "                             max_df = .57 , binary=False , token_pattern=r'\\w+' , sublinear_tf=False)\n",
    "tfidftr=vectorizertr.fit_transform(corpustr).todense()\n",
    "corpusts = testdf['ingredients_string']\n",
    "vectorizerts = TfidfVectorizer(stop_words='english')\n",
    "tfidfts=vectorizertr.transform(corpusts)\n",
    "\n",
    "predictors_tr = tfidftr\n",
    "\n",
    "targets_tr = traindf['cuisine']\n",
    "\n",
    "predictors_ts = tfidfts\n",
    "\n",
    "\n",
    "#classifier = LinearSVC(C=0.80, penalty=\"l2\", dual=False)\n",
    "parameters = {'C':[1, 10]}\n",
    "#clf = LinearSVC()\n",
    "clf = LogisticRegression()\n",
    "\n",
    "classifier = grid_search.GridSearchCV(clf, parameters)\n",
    "\n",
    "classifier=clf.fit(predictors_tr,targets_tr)\n",
    "\n",
    "testdf = pd.read_json(\"../data/test.json/test.json\") \n",
    "testdf['ingredients_clean_string'] = [' , '.join(z).strip() for z in testdf['ingredients']]\n",
    "testdf['ingredients_string'] = [' '.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]', ' ', line)) for line in lists]).strip() for lists in testdf['ingredients']]       \n",
    "\n",
    "\n",
    "predictions=classifier.predict(predictors_ts)\n",
    "testdf['cuisine'] = predictions\n",
    "testdf = testdf.sort('id' , ascending=True)\n",
    "\n",
    "#testdf[['id' , 'ingredients_clean_string' , 'cuisine' ]].to_csv(\"submission.csv\")\n",
    "\n",
    "result = predictions\n",
    "\n",
    "output = pd.DataFrame( data={\"id\":testdf[\"id\"], \"cuisine\":result} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv( \"test_model.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
